{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "vd7w2gy9ncq",
   "metadata": {},
   "source": [
    "# Notebook 05: Dashboard Integration & Deployment Readiness\n",
    "\n",
    "**Author:** Hector Carbajal  \n",
    "**Version:** 1.2 (Production Patch)  \n",
    "**Last Updated:** 2026-02\n",
    "\n",
    "---\n",
    "\n",
    "## Purpose\n",
    "This notebook serves as the **final gateway** before production deployment. It performs data pre-aggregation for the Streamlit dashboard and executes a health check on all exported assets to ensure sub-second loading times for stakeholders.\n",
    "\n",
    "## Inputs\n",
    "- `data/processed/trips_with_efficiency.csv` - Scored trip data\n",
    "\n",
    "## Outputs\n",
    "- `data/processed/dashboard/regional_summary.csv` - Aggregated region metrics\n",
    "- `data/processed/dashboard/hourly_efficiency.csv` - Time-series trend data\n",
    "\n",
    "## Deployment Readiness Checks\n",
    "- **Latency Check**: Are data files small enough for fast loading (< 5MB)?\n",
    "- **Memory Deep Audit**: Deep inspection of DataFrame memory usage.\n",
    "- **Aggregation Validation**: Confirming metrics preserve source-of-truth integrity.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1dc62640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Setup complete. Pre-aggregation target: data/processed/dashboard/\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Project imports\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "from src.config import PROCESSED_DIR\n",
    "\n",
    "DASHBOARD_DATA_DIR = PROCESSED_DIR / 'dashboard'\n",
    "DASHBOARD_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"âœ… Setup complete. Pre-aggregation target: {DASHBOARD_DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agg-header",
   "metadata": {},
   "source": [
    "## 1. Data Pre-Aggregation\n",
    "*Transforming raw ledgers into optimized dashboard assets.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "agg-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Regional summary exported.\n",
      "âœ… Hourly efficiency trend exported.\n"
     ]
    }
   ],
   "source": [
    "trips_df = pd.read_csv(PROCESSED_DIR / 'trips_with_efficiency.csv')\n",
    "\n",
    "# Regional Summary\n",
    "regional_summary = trips_df.groupby('region').agg({\n",
    "    'trip_id': 'count',\n",
    "    'efficiency_index': 'mean',\n",
    "    'is_cancelled': 'sum',\n",
    "    'is_late_pickup': 'sum'\n",
    "}).rename(columns={'trip_id': 'total_trips'})\n",
    "regional_summary['cancellation_rate'] = regional_summary['is_cancelled'] / regional_summary['total_trips']\n",
    "regional_summary.to_csv(DASHBOARD_DATA_DIR / 'regional_summary.csv')\n",
    "\n",
    "# Hourly Efficiency Trend\n",
    "hourly_efficiency = trips_df.groupby('scheduled_hour')['efficiency_index'].mean().reset_index()\n",
    "hourly_efficiency.to_csv(DASHBOARD_DATA_DIR / 'hourly_efficiency.csv', index=False)\n",
    "\n",
    "print(\"âœ… Regional summary exported.\")\n",
    "print(\"âœ… Hourly efficiency trend exported.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "audit-header",
   "metadata": {},
   "source": [
    "## 2. Dashboard Health & Integrity Audit\n",
    "*Deep inspection of export assets.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93c1dbd",
   "metadata": {},
   "outputs": [],
   "source": "print(\"ðŸ” ASSET AUDIT:\")\n\n# Store audit results for insights computation\naudit_results = {}\n\ndef verify_export(df, name, is_aggregated=True):\n    \"\"\"Deep memory inspection with comprehensive metrics.\"\"\"\n    mem_bytes = df.memory_usage(deep=True).sum()\n    size_mb = mem_bytes / (1024 * 1024)\n    row_count = len(df)\n    col_count = len(df.columns)\n    status = \"âš¡ FAST\" if size_mb < 5 else \"âš ï¸ SLOW\"\n    print(f\"- {name:20} | {row_count:>6,} rows | {col_count:>2} cols | {size_mb:>6.3f} MB | {status}\")\n    audit_results[name] = {'size_mb': size_mb, 'rows': row_count, 'cols': col_count}\n    return size_mb\n\n# Audit aggregated files\nregional_size = verify_export(regional_summary, \"Regional Summary\")\nhourly_size = verify_export(hourly_efficiency, \"Hourly Efficiency\")\n\n# Calculate actual source file size for comparison\nsource_size_mb = trips_df.memory_usage(deep=True).sum() / (1024 * 1024)\ntotal_aggregated_size = regional_size + hourly_size\nreduction_pct = ((source_size_mb - total_aggregated_size) / source_size_mb) * 100 if source_size_mb > 0 else 0\n\nprint(f\"\\nðŸ“Š AGGREGATION IMPACT:\")\nprint(f\"- Source data size:     {source_size_mb:.3f} MB ({len(trips_df):,} rows)\")\nprint(f\"- Aggregated data size: {total_aggregated_size:.3f} MB\")\nprint(f\"- Memory reduction:     {reduction_pct:.1f}%\")\n\n# Store for insights cell\ndeployment_metrics = {\n    'source_size_mb': source_size_mb,\n    'aggregated_size_mb': total_aggregated_size,\n    'reduction_pct': reduction_pct,\n    'source_rows': len(trips_df),\n    'all_fast': regional_size < 5 and hourly_size < 5\n}"
  },
  {
   "cell_type": "markdown",
   "id": "findings-header",
   "metadata": {},
   "source": [
    "## ðŸ’¡ Key Findings\n",
    "**Deployment Strategy for NEMT Dashboard**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "findings-code",
   "metadata": {},
   "outputs": [],
   "source": "# Use actual computed metrics for insights\nprint(\"=\"*80)\nprint(\"ðŸš€ DASHBOARD DEPLOYMENT PLAN\")\nprint(\"=\"*80)\n\nprint(f\"\\n1. PERFORMANCE:\")\nprint(f\"   Pre-aggregation reduces dashboard memory by {deployment_metrics['reduction_pct']:.0f}%\")\nprint(f\"   ({deployment_metrics['source_size_mb']:.2f} MB â†’ {deployment_metrics['aggregated_size_mb']:.3f} MB)\")\nprint(f\"   All assets pass latency check: {'âœ… YES' if deployment_metrics['all_fast'] else 'âŒ NO'}\")\n\nprint(f\"\\n2. DATA REFRESH:\")\nprint(f\"   Source data: {deployment_metrics['source_rows']:,} trip records\")\nprint(f\"   Recommendation: Schedule this notebook daily at 02:00 AM UTC for fresh aggregations\")\n\nprint(f\"\\n3. STAKEHOLDER ACCESS:\")\nprint(f\"   Dashboard assets exported to: {DASHBOARD_DATA_DIR}\")\nprint(f\"   The 'Driver Coaching' view is ready for HR consumption\")\nprint(f\"   Credentials: Managed via Streamlit Secrets (st.secrets)\")\n\nprint(\"\\n\" + \"=\"*80)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modivcare-venv",
   "language": "python",
   "name": "modivcare-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}